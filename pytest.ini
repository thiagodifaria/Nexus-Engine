# Pytest Configuration - Nexus Engine
# Implementei este arquivo para configurar pytest com coverage e opções otimizadas
# Decidi incluir markers para diferentes tipos de testes e configuração de output

[pytest]
# Test discovery patterns
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Paths to search for tests
testpaths =
    scripts/tests/unit
    scripts/tests/integration
    tests/python

# Minimum Python version
minversion = 3.11

# Add project root to Python path
pythonpath = .

# Command line options
addopts =
    # Verbose output
    -v
    # Show local variables in tracebacks
    --showlocals
    # Show summary of all test outcomes
    -ra
    # Enable strict markers (fail on unknown markers)
    --strict-markers
    # Disable cacheprovider for clean runs
    -p no:cacheprovider
    # Color output
    --color=yes
    # Coverage options
    --cov=backend/python/src
    --cov=frontend/src
    --cov-report=html:coverage_html
    --cov-report=term-missing
    --cov-report=json:coverage.json
    # Fail if coverage below 80%
    --cov-fail-under=80
    # Show durations of slowest tests
    --durations=10
    # Warnings
    -W ignore::DeprecationWarning
    -W ignore::PendingDeprecationWarning

# Test markers
# Implementei estes markers para organizar testes por categoria
markers =
    unit: Unit tests (fast, isolated, mocked dependencies)
    integration: Integration tests (multiple components, may use DB)
    e2e: End-to-end tests (full system, slow)
    slow: Tests that take significant time to run
    fast: Tests that run quickly (< 1s)
    cpp: Tests involving C++ engine
    api: Tests involving external API calls
    database: Tests requiring database
    websocket: Tests involving WebSocket connections
    critical: Critical path tests that must pass
    smoke: Smoke tests for quick validation
    regression: Regression tests for bug fixes
    performance: Performance/benchmark tests

# Logging configuration
log_cli = false
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = tests.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(name)s - %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Timeout configuration (requires pytest-timeout)
timeout = 300
timeout_method = thread

# Parallel execution (requires pytest-xdist)
# Use: pytest -n auto
# addopts += -n auto

# Filtering options
# filterwarnings =
#     error::UserWarning
#     ignore::DeprecationWarning

# Doctest configuration
doctest_optionflags =
    NORMALIZE_WHITESPACE
    ELLIPSIS

# Console output format
console_output_style = progress

# Notas de Uso:
#
# Executar todos os testes:
#   pytest
#
# Executar testes por marker:
#   pytest -m unit
#   pytest -m "unit and not slow"
#   pytest -m "integration or e2e"
#
# Executar testes em paralelo:
#   pytest -n auto
#
# Executar testes específicos:
#   pytest scripts/tests/unit/test_market_data_service.py
#   pytest scripts/tests/unit/test_market_data_service.py::TestMarketDataService::test_fetch_daily_data_success
#
# Ver coverage detalhado:
#   pytest --cov-report=html
#   # Abrir: coverage_html/index.html
#
# Executar apenas testes rápidos:
#   pytest -m fast
#
# Executar testes críticos:
#   pytest -m critical
#
# Pular testes lentos:
#   pytest -m "not slow"
#
# Executar com verbose máximo:
#   pytest -vv
#
# Parar no primeiro erro:
#   pytest -x
#
# Executar último teste que falhou:
#   pytest --lf
#
# Executar testes que falharam ou não rodaram:
#   pytest --lf --ff
